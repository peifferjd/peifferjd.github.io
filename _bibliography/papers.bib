---
---

@string{aps = {American Physical Society,}}

@article{bryson_enhanced_2023,
	title = {Enhanced selectivity of transcutaneous spinal cord stimulation by multielectrode configuration},
	author = {Bryson, Noah and Lombardi, Lorenzo and Hawthorn, Rachel and Fei, Jie and Keesey, Rodolfo and Peiffer, J.D. and Seáñez, Ismael},
  journal={Journal of Neural Engineering},
	doi = {10.1088/1741-2552/ace552},
  number={4},
	pages = {046015},
	date = {2023-08-01},
  year = {2023},
  preview={tscs_fig.png}
}

@inproceedings{peiffer_fusing_2024,
	title = {Fusing Uncalibrated {IMUs} and Handheld Smartphone Video to Reconstruct Knee Kinematics},
	url = {https://ieeexplore.ieee.org/document/10719724},
	doi = {10.1109/BioRob60516.2024.10719724},
	abstract = {Video and wearable sensor data provide complementary information about human movement. Video provides a holistic understanding of the entire body in the world while wearable sensors, namely inertial measurement units ({IMUs}), provide high-resolution measurements of specific body segments. A robust method to fuse these modalities and obtain biomechanically accurate kinematics would have substantial utility for clinical assessment and monitoring. Although multiple video-sensor fusion methods exist, most assume that a time-intensive and often brittle sensor-body calibration process has already been completed. In this work, we employ an implicit function to combine handheld smartphone video and uncalibrated {IMU} data at their full temporal resolution. Our monocular, video-only, biomechanical reconstruction already performs well, with only 3.91 (1.55) degrees of mean adjusted angular error in knee kinematics across 60 recordings. Re-constructing from a fusion of video and {IMU} data reduces this error to 2.9 (1.27) degrees. We validate this method in a diverse group including individuals with no gait impairments, lower limb prosthesis users, and those with a history of stroke. We also show that {IMU} data allows accurate tracking through periods of visual occlusion, equivalent to video-only.},
	eventtitle = {2024 10th {IEEE} {RAS}/{EMBS} International Conference for Biomedical Robotics and Biomechatronics ({BioRob})},
	pages = {1275--1282},
	booktitle = {2024 10th {IEEE} {RAS}/{EMBS} International Conference for Biomedical Robotics and Biomechatronics ({BioRob})},
	author = {Peiffer, J.D. and Shah, Kunal and Anarwala, Shawana and Abdou, Kayan and Cotton, R. James},
	urldate = {2024-10-25},
	date = {2024-09},
	note = {{ISSN}: 2155-1782},
	keywords = {Biomechanics, Kinematics, Accuracy, Calibration, Fuses, Knee, Robot sensing systems, Three-dimensional displays, Visualization, Wearable sensors},
  year = {2024},
  preview={fusing_fig.png},
  selected={true}
}

@inproceedings{cotton_self-supervised_2023,
	title = {Self-Supervised Learning of Gait-Based Biomarkers},
	url = {https://link.springer.com/chapter/10.1007/978-3-031-46005-0_24},
	doi = {10.1007/978-3-031-46005-0_24},
	eventtitle = {International Workshop on PRedictive Intelligence In MEdicine},
	abstract = {Markerless motion capture ({MMC}) is revolutionizing gait analysis in clinical settings by making it more accessible, raising the question of how to extract the most clinically meaningful information from gait data. In multiple fields ranging from image processing to natural language processing, self-supervised learning ({SSL}) from large amounts of unannotated data produces very effective representations for downstream tasks. However, there has only been limited use of {SSL} to learn effective representations of gait and movement, and it has not been applied to gait analysis with {MMC}. One {SSL} objective that has not been applied to gait is contrastive learning, which finds representations that place similar samples closer together in the learned space. If the learned similarity metric captures clinically meaningful differences, this could produce a useful representation for many downstream clinical tasks. Contrastive learning can also be combined with causal masking to predict future timesteps, which is an appealing {SSL} objective given the dynamical nature of gait. We applied these techniques to gait analyses performed with {MMC} in a rehabilitation hospital from a diverse clinical population. We find that contrastive learning on unannotated gait data learns a representation that captures clinically meaningful information. We probe this learned representation using the framework of biomarkers and show it holds promise as both a diagnostic and response biomarker, by showing it can accurately classify diagnosis from gait and is responsive to inpatient therapy, respectively. We ultimately hope these learned representations will enable predictive and prognostic gait-based biomarkers that can facilitate precision rehabilitation through greater use of {MMC} to quantify movement in rehabilitation.},
	pages = {277--291},
	booktitle = {Predictive Intelligence in Medicine},
	author = {Cotton, R. James and Peiffer, J.D. and Shah, Kunal and DeLillo, Allison and Cimorelli, Anthony and Anarwala, Shawana and Abdou, Kayan and Karakostas, Tasos},
	date = {2023},
	keywords = {gait analysis, rehabilitation, contrastive learning, gait biomarkers, markerless motion capture, self-supervised learning},
	year = {2023},
	preview={ssl_fig.png},
	selected={true}
}

@article{peiffer_hyperpolarized_2024,
	title = {Hyperpolarized 129Xe MRI, 99mTc scintigraphy, and SPECT in lung ventilation imaging: a quantitative comparison},
	author = {Peiffer, J.D. and Altes, Talissa and Ruset, Iulian C. and Hersman, F. W. and Mugler, John P. and Meyer, Craig H. and Mata, Jamie and Qing, Kun and Thomen, Robert},
	journal = {Academic Radiology},
	doi = {10.1016/j.acra.2023.10.038},
  number={4},
	pages = {1666--1675},
	date = {2024-04-01},
  year = {2024},
  preview = {hyperpolarized_fig.png}
}

@misc{unger_differentiable_2024,
	title = {Differentiable Biomechanics for Markerless Motion Capture in Upper Limb Stroke Rehabilitation: A Comparison with Optical Motion Capture},
	url = {http://arxiv.org/abs/2411.14992},
	doi = {10.48550/arXiv.2411.14992},
	shorttitle = {Differentiable Biomechanics for Markerless Motion Capture in Upper Limb Stroke Rehabilitation},
	abstract = {Marker-based Optical Motion Capture ({OMC}) paired with biomechanical modeling is currently considered the most precise and accurate method for measuring human movement kinematics. However, combining differentiable biomechanical modeling with Markerless Motion Capture ({MMC}) offers a promising approach to motion capture in clinical settings, requiring only minimal equipment, such as synchronized webcams, and minimal effort for data collection. This study compares key kinematic outcomes from biomechanically modeled {MMC} and {OMC} data in 15 stroke patients performing the drinking task, a functional task recommended for assessing upper limb movement quality. We observed a high level of agreement in kinematic trajectories between {MMC} and {OMC}, as indicated by high correlations (median r above 0.95 for the majority of kinematic trajectories) and median {RMSE} values ranging from 2-5 degrees for joint angles, 0.04 m/s for end-effector velocity, and 6 mm for trunk displacement. Trial-to-trial biases between {OMC} and {MMC} were consistent within participant sessions, with interquartile ranges of bias around 1-3 degrees for joint angles, 0.01 m/s in end-effector velocity, and approximately 3mm for trunk displacement. Our findings indicate that our {MMC} for arm tracking is approaching the accuracy of marker-based methods, supporting its potential for use in clinical settings. {MMC} could provide valuable insights into movement rehabilitation in stroke patients, potentially enhancing the effectiveness of rehabilitation strategies.},
	number = {{arXiv}:2411.14992},
	publisher = {{arXiv}},
	author = {Unger, Tim and Moslehian, Arash Sal and Peiffer, J.D. and Ullrich, Johann and Gassert, Roger and Lambercy, Olivier and Cotton, R. James and Easthope, Chris Awai},
	urldate = {2024-11-25},
	date = {2024-11-22},
	eprinttype = {arxiv},
	eprint = {2411.14992},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	year = {2024},
	preview = {llui_fig.png}
}

@inproceedings{firouzabadi_biomechanical_2024,
	title = {Biomechanical Arm and Hand Tracking with Multiview Markerless Motion Capture},
	url = {https://ieeexplore.ieee.org/document/10719940/?arnumber=10719940},
	doi = {10.1109/BioRob60516.2024.10719940},
	abstract = {Human arm and hand function is extremely complex with many degrees of freedom. It is also a common target for clinical interventions. However, precisely measuring upper extremity movement in both clinical and research settings is logistically challenging. We overcame this challenge through a novel approach to reconstructing arm biomechanics from markerless motion capture from multiple synchronized videos. Our approach directly opti-mizes the kinematics of an accurate biomechanical arm and hand that allows end-to-end minimization of the errors between the reconstructed movements and keypoints detected by computer vision. Key to this is an implicit function that maps from time to joint kinematics, which provides a learnable trajectory representation that can be differentiated through the biomechanical model, and supports {GPU} acceleration using {MuJoCo}-{MJX}. This optimization solves for the inverse kinematic solution consistent with the measured keypoints, consistent with biomechanical constraints, in addition to scaling the model while solving for the kinematics. We compare different hand keypoint detectors and find the best produces a fit with only several millimeters of reconstruction error. We also find that end-to-end optimization outperforms a two-stage fitting procedure, equivalent to more traditional biomechanical pipelines, where we first compute 3D marker trajectories and then perform inverse kinematics fitting in {OpenSim}. We anticipate this framework will reduce the barriers to biomechanical analysis of the arm and hand in both clinical and research settings.},
	eventtitle = {2024 10th {IEEE} {RAS}/{EMBS} International Conference for Biomedical Robotics and Biomechatronics ({BioRob})},
	pages = {1641--1648},
	booktitle = {2024 10th {IEEE} {RAS}/{EMBS} International Conference for Biomedical Robotics and Biomechatronics ({BioRob})},
	author = {Firouzabadi, Pouyan and Murray, Wendy and Sobinov, Anton R and Peiffer, J.D. and Shah, Kunal and Miller, Lee E and Cotton, R. James},
	urldate = {2024-11-28},
	date = {2024-09},
	note = {{ISSN}: 2155-1782},
	keywords = {Biomechanics, Motion capture, Computational modeling, Optimization, Biological system modeling, Fitting, Graphics processing units, Tracking, Trajectory, Videos},
	file = {Full Text PDF:C\:\\Users\\jdpei\\Zotero\\storage\\54KV32LK\\Firouzabadi et al. - 2024 - Biomechanical Arm and Hand Tracking with Multiview Markerless Motion Capture.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\jdpei\\Zotero\\storage\\52HETLFD\\10719940.html:text/html},
	year = {2024},
}

@misc{cotton_markerless_2023,
	title = {Markerless Motion Capture and Biomechanical Analysis Pipeline},
	url = {http://arxiv.org/abs/2303.10654},
	abstract = {Markerless motion capture using computer vision and human pose estimation ({HPE}) has the potential to expand access to precise movement analysis. This could greatly beneﬁt rehabilitation by enabling more accurate tracking of outcomes and providing more sensitive tools for research. There are numerous steps between obtaining videos to extracting accurate biomechanical results and limited research to guide many critical design decisions in these pipelines. In this work, we analyze several of these steps including the algorithm used to detect keypoints and the keypoint set, the approach to reconstructing trajectories for biomechanical inverse kinematics and optimizing the {IK} process. Several features we ﬁnd important are: 1) using a recent algorithm trained on many datasets that produces a dense set of biomechanically-motivated keypoints, 2) using an implicit representation to reconstruct smooth, anatomically constrained marker trajectories for {IK}, 3) iteratively optimizing the biomechanical model to match the dense markers, 4) appropriate regularization of the {IK} process. Our pipeline makes it easy to obtain accurate biomechanical estimates of movement in a rehabilitation hospital.},
	number = {{arXiv}:2303.10654},
	publisher = {{arXiv}},
	author = {Cotton, R. James and {DeLillo}, Allison and Cimorelli, Anthony and Shah, Kunal and Peiffer, J. D. and Anarwala, Shawana and Abdou, Kayan and Karakostas, Tasos},
	urldate = {2024-06-19},
	date = {2023-03-19},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2303.10654 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	year = {2023},
}

@inproceedings{cotton_optimizing_2023,
	location = {Singapore, Singapore},
	title = {Optimizing Trajectories and Inverse Kinematics for Biomechanical Analysis of Markerless Motion Capture Data},
	isbn = {979-8-3503-4275-8},
	url = {https://ieeexplore.ieee.org/document/10304683/},
	doi = {10.1109/ICORR58425.2023.10304683},
	abstract = {Markerless motion capture using computer vision and human pose estimation ({HPE}) has the potential to expand access to precise movement analysis. This could greatly benefit rehabilitation by enabling more accurate tracking of outcomes and providing more sensitive tools for research. There are numerous steps between obtaining videos to extracting accurate biomechanical results and limited research to guide many critical design decisions in these pipelines. In this work, we analyze several of these steps including the algorithm used to detect keypoints and the keypoint set, the approach to reconstructing trajectories for biomechanical inverse kinematics and optimizing the {IK} process. Several features we find important are: 1) using a recent algorithm trained on many datasets that produces a dense set of biomechanically-motivated keypoints, 2) using an implicit representation to reconstruct smooth, anatomically constrained marker trajectories for {IK}, 3) iteratively optimizing the biomechanical model to match the dense markers, 4) appropriate regularization of the {IK} process. Our pipeline makes it easy to obtain accurate biomechanical estimates of movement in a rehabilitation hospital.},
	eventtitle = {2023 International Conference on Rehabilitation Robotics ({ICORR})},
	pages = {1--6},
	booktitle = {2023 International Conference on Rehabilitation Robotics ({ICORR})},
	publisher = {{IEEE}},
	author = {Cotton, R. James and {DeLillo}, Allison and Cimorelli, Anthony and Shah, Kunal and Peiffer, J.D. and Anarwala, Shawana and Abdou, Kayan and Karakostas, Tasos},
	urldate = {2023-11-17},
	date = {2023-09-24},
	langid = {english},
	year={2023},
}