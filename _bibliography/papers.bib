---
---

@string{aps = {American Physical Society,}}

@article{bryson_enhanced_2023,
	title = {Enhanced selectivity of transcutaneous spinal cord stimulation by multielectrode configuration},
	author = {Bryson, Noah and Lombardi, Lorenzo and Hawthorn, Rachel and Fei, Jie and Keesey, Rodolfo and Peiffer, J.D. and Seáñez, Ismael},
  journal={Journal of Neural Engineering},
	doi = {10.1088/1741-2552/ace552},
  number={4},
	pages = {046015},
	date = {2023-08-01},
  year = {2023},
  preview={tscs_fig.png}
}

@inproceedings{peiffer_fusing_2024,
	title = {Fusing Uncalibrated {IMUs} and Handheld Smartphone Video to Reconstruct Knee Kinematics},
	url = {https://ieeexplore.ieee.org/document/10719724},
	doi = {10.1109/BioRob60516.2024.10719724},
	abstract = {Video and wearable sensor data provide comple-mentary information about human movement. Video provides a holistic understanding of the entire body in the world while wearable sensors, namely inertial measurement units ({IMUs}), provide high-resolution measurements of specific body segments. A robust method to fuse these modalities and obtain biomechanically accurate kinematics would have substantial utility for clinical assessment and monitoring. Although multiple video-sensor fusion methods exist, most assume that a time-intensive and often brittle sensor-body calibration process has already been completed. In this work, we employ an implicit function to combine handheld smartphone video and uncalibrated {IMU} data at their full temporal resolution. Our monocular, video-only, biomechanical reconstruction already performs well, with only 3.91 (1.55) degrees of mean adjusted angular error in knee kinematics across 60 recordings. Re-constructing from a fusion of video and {IMU} data reduces this error to 2.9 (1.27) degrees. We validate this method in a diverse group including individuals with no gait impairments, lower limb prosthesis users, and those with a history of stroke. We also show that {IMU} data allows accurate tracking through periods of visual occlusion, equivalent to video-only.},
	eventtitle = {2024 10th {IEEE} {RAS}/{EMBS} International Conference for Biomedical Robotics and Biomechatronics ({BioRob})},
	pages = {1275--1282},
	booktitle = {2024 10th {IEEE} {RAS}/{EMBS} International Conference for Biomedical Robotics and Biomechatronics ({BioRob})},
	author = {Peiffer, J.D. and Shah, Kunal and Anarwala, Shawana and Abdou, Kayan and Cotton, R. James},
	urldate = {2024-10-25},
	date = {2024-09},
	note = {{ISSN}: 2155-1782},
	keywords = {Biomechanics, Kinematics, Accuracy, Calibration, Fuses, Knee, Robot sensing systems, Three-dimensional displays, Visualization, Wearable sensors},
  year = {2024},
  preview={fusing_fig.png},
  selected={true}
}

@article{peiffer_hyperpolarized_2024,
	title = {Hyperpolarized 129Xe MRI, 99mTc scintigraphy, and SPECT in lung ventilation imaging: a quantitative comparison},
	author = {Peiffer, J.D. and Altes, Talissa and Ruset, Iulian C. and Hersman, F. W. and Mugler, John P. and Meyer, Craig H. and Mata, Jamie and Qing, Kun and Thomen, Robert},
	journal = {Academic Radiology},
	doi = {10.1016/j.acra.2023.10.038},
  number={4},
	pages = {1666--1675},
	date = {2024-04-01},
  year = {2024},
}

@inproceedings{cotton_self-supervised_2023,
	title = {Self-Supervised Learning of Gait-Based Biomarkers},
	isbn = {978-3-031-46005-0},
	doi = {10.1007/978-3-031-46005-0_24},
	abstract = {Markerless motion capture ({MMC}) is revolutionizing gait analysis in clinical settings by making it more accessible, raising the question of how to extract the most clinically meaningful information from gait data. In multiple fields ranging from image processing to natural language processing, self-supervised learning ({SSL}) from large amounts of unannotated data produces very effective representations for downstream tasks. However, there has only been limited use of {SSL} to learn effective representations of gait and movement, and it has not been applied to gait analysis with {MMC}. One {SSL} objective that has not been applied to gait is contrastive learning, which finds representations that place similar samples closer together in the learned space. If the learned similarity metric captures clinically meaningful differences, this could produce a useful representation for many downstream clinical tasks. Contrastive learning can also be combined with causal masking to predict future timesteps, which is an appealing {SSL} objective given the dynamical nature of gait. We applied these techniques to gait analyses performed with {MMC} in a rehabilitation hospital from a diverse clinical population. We find that contrastive learning on unannotated gait data learns a representation that captures clinically meaningful information. We probe this learned representation using the framework of biomarkers and show it holds promise as both a diagnostic and response biomarker, by showing it can accurately classify diagnosis from gait and is responsive to inpatient therapy, respectively. We ultimately hope these learned representations will enable predictive and prognostic gait-based biomarkers that can facilitate precision rehabilitation through greater use of {MMC} to quantify movement in rehabilitation.},
	pages = {277--291},
	booktitle = {Predictive Intelligence in Medicine},
	publisher = {Springer Nature Switzerland},
	author = {Cotton, R. James and Peiffer, J. D. and Shah, Kunal and {DeLillo}, Allison and Cimorelli, Anthony and Anarwala, Shawana and Abdou, Kayan and Karakostas, Tasos},
	editor = {Rekik, Islem and Adeli, Ehsan and Park, Sang Hyun and Cintas, Celia and Zamzmi, Ghada},
	date = {2023},
	langid = {english},
	keywords = {gait analysis, rehabilitation, contrastive learning, gait biomarkers, markerless motion capture, self-supervised learning},
}
